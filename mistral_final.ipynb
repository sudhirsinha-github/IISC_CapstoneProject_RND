{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12359208,
          "sourceType": "datasetVersion",
          "datasetId": 7792059
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers accelerate datasets peft trl\n",
        "#Restart after"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:19.587718Z",
          "iopub.execute_input": "2025-07-03T12:51:19.587953Z",
          "iopub.status.idle": "2025-07-03T12:51:26.403754Z",
          "shell.execute_reply.started": "2025-07-03T12:51:19.587935Z",
          "shell.execute_reply": "2025-07-03T12:51:26.403019Z"
        },
        "id": "rx-S-80MXaoK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:26.404658Z",
          "iopub.execute_input": "2025-07-03T12:51:26.404875Z",
          "iopub.status.idle": "2025-07-03T12:51:26.740259Z",
          "shell.execute_reply.started": "2025-07-03T12:51:26.404853Z",
          "shell.execute_reply": "2025-07-03T12:51:26.739357Z"
        },
        "id": "r-2_JwS7XaoL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "#https://huggingface.co/mistralai/Mistral-7B-v0.1 -> accept\n",
        "#Login and create token\n",
        "login(token=\"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\")\n",
        "#snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.1\", token=\"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\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:26.741140Z",
          "iopub.execute_input": "2025-07-03T12:51:26.741482Z",
          "iopub.status.idle": "2025-07-03T12:51:36.095580Z",
          "shell.execute_reply.started": "2025-07-03T12:51:26.741463Z",
          "shell.execute_reply": "2025-07-03T12:51:36.094773Z"
        },
        "id": "URvp7skXXaoL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset_path = \"/kaggle/input/datasetz/labeled_2025-06-28.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:36.097440Z",
          "iopub.execute_input": "2025-07-03T12:51:36.097924Z",
          "iopub.status.idle": "2025-07-03T12:51:36.411265Z",
          "shell.execute_reply.started": "2025-07-03T12:51:36.097905Z",
          "shell.execute_reply": "2025-07-03T12:51:36.410709Z"
        },
        "id": "2NlRdlLWXaoM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and tokenizer name\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:36.411848Z",
          "iopub.execute_input": "2025-07-03T12:51:36.412020Z",
          "iopub.status.idle": "2025-07-03T12:51:36.736827Z",
          "shell.execute_reply.started": "2025-07-03T12:51:36.412004Z",
          "shell.execute_reply": "2025-07-03T12:51:36.736008Z"
        },
        "id": "GcCSVfLNXaoM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Format function\n",
        "def format_example(example):\n",
        "    user_msg = example[\"messages\"][0][\"content\"]\n",
        "    labels = {\n",
        "        \"intent\": \", \".join(example[\"intent\"]),\n",
        "        \"sentiment\": \", \".join(example[\"sentiment\"]),\n",
        "        \"topic\": \", \".join(example[\"topic\"])\n",
        "    }\n",
        "    prompt = f\"<s>[INST] {user_msg} [/INST]\"\n",
        "    output = f\"Intent: {labels['intent']}\\nSentiment: {labels['sentiment']}\\nTopic: {labels['topic']}</s>\"\n",
        "    return {\"text\": prompt + output}\n",
        "\n",
        "dataset = dataset.map(format_example)\n",
        "\n",
        "# Tokenization\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:36.737754Z",
          "iopub.execute_input": "2025-07-03T12:51:36.738347Z",
          "iopub.status.idle": "2025-07-03T12:51:36.801309Z",
          "shell.execute_reply.started": "2025-07-03T12:51:36.738320Z",
          "shell.execute_reply": "2025-07-03T12:51:36.800677Z"
        },
        "id": "k6i6OQxqXaoM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load quantized model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:51:36.802061Z",
          "iopub.execute_input": "2025-07-03T12:51:36.802341Z",
          "iopub.status.idle": "2025-07-03T12:51:55.629058Z",
          "shell.execute_reply.started": "2025-07-03T12:51:36.802323Z",
          "shell.execute_reply": "2025-07-03T12:51:55.628210Z"
        },
        "id": "M7detKjhXaoM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-lora-quant\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=1,\n",
        "    resume_from_checkpoint=True,\n",
        "    gradient_accumulation_steps=4, #6435 if 2\n",
        "    label_names=[\"input_ids\"]\n",
        ")\n",
        "#    logging_steps=100,\n",
        "#    save_steps=1000,\n",
        "#  save_total_limit=1,\n",
        "# gradient_accumulation_steps=4 -> reduce memory pressure\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:54:31.662728Z",
          "iopub.execute_input": "2025-07-03T12:54:31.663307Z",
          "iopub.status.idle": "2025-07-03T12:54:31.706685Z",
          "shell.execute_reply.started": "2025-07-03T12:54:31.663285Z",
          "shell.execute_reply": "2025-07-03T12:54:31.706115Z"
        },
        "id": "UE1f41ojXaoM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer.train()\n",
        "#trainer.train(resume_from_checkpoint=\"./mistral-lora-quant\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:54:38.302966Z",
          "iopub.execute_input": "2025-07-03T12:54:38.303590Z"
        },
        "id": "PZNnFzUqXaoM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a random test sample\n",
        "import random\n",
        "sample = random.choice(dataset[\"test\"])\n",
        "input_prompt = f\"<s>[INST] {sample['messages'][0]['content']} [/INST]\"\n",
        "\n",
        "inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(\"Generated:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\nGround truth:\")\n",
        "print(f\"Intent: {sample['intent']}\")\n",
        "print(f\"Sentiment: {sample['sentiment']}\")\n",
        "print(f\"Topic: {sample['topic']}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:52:30.910099Z",
          "iopub.status.idle": "2025-07-03T12:52:30.910465Z",
          "shell.execute_reply.started": "2025-07-03T12:52:30.910289Z",
          "shell.execute_reply": "2025-07-03T12:52:30.910305Z"
        },
        "id": "a2XtXG8CXaoM"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}